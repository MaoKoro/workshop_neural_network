{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Vous devez normalement mieux connaître cette librairie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Te souviens-tu comment lire un csv ?\n",
    "data = pd.read_csv(\"./clean_weather.csv\", index_col=0)\n",
    "data = data.ffill()\n",
    "data.plot.scatter(\"tmax\", \"tmax_tomorrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data.plot.scatter(\"tmax\", \"tmax_tomorrow\")\n",
    "\n",
    "prediction = lambda x, w1=.82, b=11.99: x * w1 + b\n",
    "\n",
    "plt.plot([30, 120], [prediction(30), prediction(120)], 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse(actual, predicted):\n",
    "    return np.mean((actual - predicted) ** 2)\n",
    "\n",
    "print(mse(data[\"tmax_tomorrow\"], prediction(data[\"tmax\"])))\n",
    "print(mse(data[\"tmax_tomorrow\"], prediction(data[\"tmax\"], .82, 13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_bins = pd.cut(data[\"tmax\"], 25) # Classer les données dans des bacs\n",
    "tmax_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = (data[\"tmax_tomorrow\"] - 11.99) / data[\"tmax\"]\n",
    "binned_ratio = ratios.groupby(tmax_bins, observed=True).mean()\n",
    "\n",
    "binned_tmax = data[\"tmax\"].groupby(tmax_bins, observed=True).mean()\n",
    "plt.scatter(binned_tmax, binned_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons l'apercevoir la température n'évolue pas de façon linéaire. La régression linéaire seule ne suffira pas.   \n",
    "\n",
    "Pour résoudre ce problème, notre réseau de neurones va passer par 3 étapes clés:   \n",
    "1. Une transformation non-linéaire au-dessus de la transformation linéaire\n",
    "2. Plusieurs layers, qui vont chacune récupérer les interactions entre les caractéristiques\n",
    "3. Plusieurs unités cachées par couche dont chacune a des transformations linéaires et non-linéaires\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'activation - ReLU\n",
    "**Equation de la régression linéaire:**   \n",
    "    \n",
    "$y = wx + b$\n",
    "\n",
    "non-linéaire    \n",
    "$y = relu(wx + b)$\n",
    "\n",
    "*w = weight (poids)*   \n",
    "*b = bias (biais)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = np.arange(-50, 50) # Plage d'intervalle de test compris entre -50 et 50\n",
    "\n",
    "plt.plot(temps, np.maximum(0, prediction(temps))) # Représentation graphique de notre fonction d'activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre fonction d'activation n'est plutôt pas mal mais il y a encore un soucis car dans l'exemple du graphique précédent, pour une température de -40° nous allons prédire 0° pour le lendemain. Ce qui sera généralement une très mauvaise prédiction !\n",
    "\n",
    "C'est donc pour cela que notre réseau de neurones aura besoin de plusieurs couches.\n",
    "\n",
    "$\\hat{y} = w_{2} relu(w_{1}x + b_{1}) + b_2$\n",
    "\n",
    "$relu(w_{1}x + b_{1})$   étant notre première couche\n",
    "\n",
    "$w_{2} * output + b_2$   est notre deuxième couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = np.arange(-50, 50) # On re initie une intervalle de test\n",
    "\n",
    "layer1 = np.maximum(0, prediction(temps)) # Première couche\n",
    "layer2 = prediction(layer1, .5, 10) # Deuxième couche ( poids = 0.5 et biais = 10)\n",
    "\n",
    "plt.plot(temps, layer2)\n",
    "\n",
    "plt.ylim((0,40)) # Limiter la taille du graphique sur l'axe y\n",
    "\n",
    "# Par la suite le réseau de neurone apprendra lui même ses poids et biais à assigner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons apercevoir une légère amélioration car lorsque la température est négative, celle prédite le lendemain sera maintenant de 10. Le biais s'ajustera seul pour trouver une valeur plus cohérente. Mais nous appliquons toujours une constante pour toute température inférieure à 0°. Pour résoudre ce problème nous allons ajouter plusieurs couches d'unités."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple hidden units\n",
    "\n",
    "### 1. Aperçu des layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_1 = np.maximum(0, prediction(temps))\n",
    "\n",
    "layer1_2 = np.maximum(0, prediction(temps, .1, 10)) # Exemple\n",
    "\n",
    "layer1_3 = np.maximum(0, prediction(temps, 2, -50)) # Exemple\n",
    "\n",
    "layer2 = layer1_1 * .1 + layer1_2 * .3 + layer1_3 * .4 + 20\n",
    "\n",
    "# plt.plot(temps, layer1_1)\n",
    "# plt.plot(temps, layer1_2)\n",
    "# plt.plot(temps, layer1_3)\n",
    "# plt.plot(temps, layer1_1 + layer1_2 + layer1_3)\n",
    "plt.plot(temps, layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons observer qu'en fonction de nos couches et de l'ajustement de nos biais notre réseau de neurones peut comprendre des interactions plus complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Création des matrices\n",
    "\n",
    "En mathématiques, les matrices sont des tableaux d'éléments qui servent à interpréter en termes calculatoires, les résultats théoriques de l'algèbre linéaire et même de l'algèbre bilinéaire.\n",
    "\n",
    "<img src=\"assets/matrices2.png\" alt=\"Exemple de matrice\" style=\"width:779px;height:401px;\">\n",
    "\n",
    "La multiplication entre matrices se passe comme ça:\n",
    "\n",
    "<img src=\"assets/matrice_mul.gif\" alt=\"Matrice mul\" style=\"width:600px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsensor import explain as exp # Cette librairie permet d'avoir plus d'infos sur les matrices calculées\n",
    "\n",
    "my_input = np.array([[80], [90], [100], [-20], [-10]])\n",
    "\n",
    "l1_weights = np.array([[.82, .1]])\n",
    "\n",
    "l1_bias = np.array([[11.99, 10]])\n",
    "\n",
    "with exp():\n",
    "    l1_output = my_input @ l1_weights + l1_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l1_output, end=\"\\n\\n\")\n",
    "l1_activated = np.maximum(l1_output, 0) # Fonction d'activation (ReLU)\n",
    "print(l1_activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre fonction d'activation a bien affecté notre matrice. La valeur négative (-4) a été mise à 0 comme le prévoit ReLU.\n",
    "\n",
    "$layer_{1}=relu(XW_{1} + B_{1})$\n",
    "\n",
    "$\\hat{y}=W_{2}relu(XW_{1} + B_{1}) + B_{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_weights = np.array([\n",
    "    [.5],\n",
    "    [.2]\n",
    "])\n",
    "\n",
    "l2_bias = np.array([[5]])\n",
    "\n",
    "with exp():\n",
    "    output = l1_activated @ l2_weights + l2_bias\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = np.array([[80], [90], [100], [-20], [-10]])\n",
    "tmax_tomorrow = np.array([[83], [89], [95], [-22], [-9]])\n",
    "tmax_tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(actual, predicted):\n",
    "    return(actual - predicted) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(tmax_tomorrow, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(actual, predicted):\n",
    "    return predicted - actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_grad(tmax_tomorrow, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation\n",
    "Mettre à jour nos poids et biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gradient = mse_grad(tmax_tomorrow, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp():\n",
    "    l2_w_gradient = l1_activated.T @ output_gradient\n",
    "\n",
    "l2_w_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour trouver les bons poids à mettre nous devont calculé la dérivé partielle de la perte par rapport à la deuxième matrice de poids.\n",
    "\n",
    "$\\frac{\\partial L}{\\partial XW_{2}}$\n",
    "La formule ci-dessus indique comment une petite variation dans la sortie de la couche cachée affecte la fonction de perte. Cette information est utilisée pour propager l'erreur de la couche de sortie vers la couche cachée lors de la rétropropagation.\n",
    "\n",
    "\n",
    "Comment la calculer:   \n",
    " \n",
    " $$\\frac{\\partial L}{\\partial W_{2}}=\\partial L\\frac{\\partial (XW_{2})}{\\partial W_{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la dérivée par rapport au biais\n",
    "with exp():\n",
    "    l2_b_gradient = np.mean(output_gradient, axis=0)\n",
    "l2_b_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser l'[algorithme du gradient (Gradient descent)](https://fr.wikipedia.org/wiki/Algorithme_du_gradient) pour mettre à jour nos poids et biais de la seconde couche.\n",
    "\n",
    "Nous allons avoir besoin d'un **learning rate (taux d'apprentissage)** qui va permettre à notre *gradient descent* de converger vers la sortie souhaité. Plus il sera haut, plus il sera difficile pour lui de converger. Plus il sera bas plus il mettra du temps à converger. Il faut donc trouver une valeur souhaitable, on ne peut la connaître que en testant différentes valeures. C'est ce qu'on appelle un **hyperparamètre**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "\n",
    "with exp():\n",
    "    l2_bias = l2_bias - l2_b_gradient * learning_rate\n",
    "    l2_weights = l2_weights - l2_w_gradient * learning_rate\n",
    "\n",
    "l2_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir calculé les poids et biais de la seconde couche nous pouvons maintenant calculer ceux de la première couche !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp():\n",
    "    l1_activated_gradient = output_gradient @ l2_weights.T\n",
    "l1_activated_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = np.arange(-50, 50)\n",
    "plt.plot(temps, np.maximum(0, temps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = np.maximum(0, temps)\n",
    "plt.plot(temps[1:], activation[1:] - np.roll(activation, 1)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp():\n",
    "    l1_output_gradient = l1_activated_gradient * np.heaviside(l1_output, 0)\n",
    "l1_output_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back-propagation\n",
    "l1_w_gradient = my_input.T @ l1_output_gradient\n",
    "l1_b_gradient = np.mean(l1_output_gradient, axis=0)\n",
    "\n",
    "# Gradient descent\n",
    "l1_weights -= l1_w_gradient * learning_rate\n",
    "l1_bias -= l1_b_gradient * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé de ce que nous avons pu voir ici...\n",
    "\n",
    "1. Lancer le forward pass, et récupérer l'output:\n",
    "2. Calculer le gradient par rapport aux sorties du réseau. (fonction `mse_grad`)\n",
    "3. Pour chaque couche (layer) du réseau:\n",
    "    - Calculer le gradient par rapport à la non-linéarité de la sortie (si la couche a de la non-linéarité)\n",
    "    - Calculer le gradient par rapport aux poids\n",
    "    - Calculer le gradient par rapport aux biais\n",
    "    - Calculer le gradient par rapport aux entrées (inputs) de la couche\n",
    "4. Mettre à jour les paramètres du réseau en utilisant le *Gradient descent*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
